[모듈 6]
# scripts/run_backtest.py

---

# scripts/run_data_delete.py
"""
이 스크립트는 데이터베이스의 모든 테이블을 삭제하는 기능을 수행합니다.
주로 테스트 환경에서 초기화를 위해 사용될 수 있으며, 
PostgreSQL의 public 스키마 내 모든 테이블을 제거합니다.
"""

import sys  # 시스템 종료 및 예외 처리를 위해 사용
from dotenv import load_dotenv  # 환경변수 로드를 위한 라이브러리
from sqlalchemy import create_engine, text  # 데이터베이스 연결 및 SQL 실행을 위한 라이브러리
from data.db.db_config import DATABASE  # 데이터베이스 접속 정보를 담은 설정 객체
from logs.log_config import initialize_root_logger, setup_logger  # 로깅 초기화 및 설정 함수

# 환경변수 로드 및 로깅 초기화
load_dotenv()  # .env 파일에 정의된 환경변수를 시스템에 로드합니다.
initialize_root_logger()  # 루트 로거를 초기화합니다.
logger = setup_logger(__name__)  # 모듈 별 로거를 생성합니다.

def drop_all_tables(db_config):
    """
    데이터베이스 내의 모든 테이블을 삭제합니다.
    
    Parameters:
        db_config (dict): 데이터베이스 접속 정보를 담은 딕셔너리 
                          (예: {'user': ..., 'password': ..., 'host': ..., 'port': ..., 'dbname': ...})
    
    Returns:
        None: 모든 테이블 삭제 작업 후 성공 메시지 혹은 에러 로그를 남깁니다.
    """
    try:
        # 데이터베이스 연결 문자열을 구성하고 SQLAlchemy 엔진을 생성합니다.
        engine = create_engine(
            f"postgresql://{db_config['user']}:{db_config['password']}@"
            f"{db_config['host']}:{db_config['port']}/{db_config['dbname']}",
            pool_pre_ping=True  # 커넥션의 유효성을 주기적으로 확인합니다.
        )
        # 데이터베이스 트랜잭션을 시작합니다. (자동 커밋 모드)
        with engine.begin() as conn:
            # public 스키마 내의 모든 테이블 이름을 조회하는 SQL 문 실행
            result = conn.execute(text("SELECT tablename FROM pg_tables WHERE schemaname = 'public'"))
            tables = [row[0] for row in result]  # 결과에서 테이블 이름만 추출하여 리스트로 생성
            if not tables:
                logger.debug("No tables found in the database.")
                return
            # 조회된 각 테이블에 대해 DROP TABLE 명령어를 실행합니다.
            for table in tables:
                logger.debug(f"Dropping table {table}...")
                conn.execute(text(f"DROP TABLE IF EXISTS {table} CASCADE"))
            logger.debug("All tables dropped successfully.")
    except Exception as e:
        # 예외 발생 시 에러 로그를 기록하고 프로그램을 종료합니다.
        logger.error(f"Error dropping tables: {e}", exc_info=True)
        sys.exit(1)

def run_drop_db_tables():
    """
    데이터베이스 접속 정보(DATABASE)를 사용하여 drop_all_tables 함수를 실행합니다.
    
    Parameters:
        없음
    
    Returns:
        None
    """
    drop_all_tables(DATABASE)

# 스크립트가 직접 실행될 경우 run_drop_db_tables 함수를 호출합니다.
if __name__ == "__main__":
    run_drop_db_tables()

---

# scripts/run_data_store.py

---

# scripts/run_data_update.py
"""
이 스크립트는 거래 데이터(OHLCV: Open, High, Low, Close, Volume)를
최신 상태로 업데이트하기 위한 작업을 수행합니다.

주요 기능:
  1. .env에 정의된 데이터베이스가 없으면 생성
  2. 상위 거래량 심볼 및 해당 심볼의 최초 온보딩 날짜를 조회하여
     공통 시작 날짜를 결정
  3. 각 심볼과 각 시간 프레임에 대해 기존 데이터를 조회하고,
     새로운 데이터를 API 등으로 가져와서 데이터베이스에 삽입
"""

import sys  # 시스템 종료를 위해 사용
from datetime import datetime, timedelta, timezone  # 날짜 및 시간 처리를 위한 모듈
import pandas as pd  # 데이터프레임 처리를 위한 라이브러리
from dotenv import load_dotenv  # 환경변수 로드를 위한 라이브러리
import psycopg2  # PostgreSQL 데이터베이스 연결 라이브러리
from psycopg2 import sql  # SQL 쿼리 작성 시 안전하게 식별자 처리를 위한 모듈

from data.db.db_config import DATABASE  # 데이터베이스 접속 정보를 담은 설정 객체
from data.db.db_manager import fetch_ohlcv_records, insert_ohlcv_records  # 기존 데이터 조회 및 삽입 함수
from data.fetcher.fetch_data import fetch_historical_ohlcv_data, get_top_volume_symbols, get_latest_onboard_date  # OHLCV 데이터 관련 함수들
from logs.log_config import initialize_root_logger, setup_logger  # 로깅 초기화 및 설정 함수

# 환경변수 로드 및 로깅 초기화
load_dotenv()
initialize_root_logger()
logger = setup_logger(__name__)

def create_database_if_not_exists(db_config):
    """
    .env에 명시된 데이터베이스가 존재하지 않을 경우,
    PostgreSQL의 기본 데이터베이스("postgres")에 접속하여 해당 데이터베이스를 생성합니다.
    
    Parameters:
        db_config (dict): 데이터베이스 접속 정보를 담은 딕셔너리 
                          (예: {'dbname': ..., 'user': ..., 'password': ..., 'host': ..., 'port': ...})
    
    Returns:
        None: 데이터베이스 생성 여부에 따라 로그를 기록하며, 에러 발생 시 프로그램을 종료합니다.
    """
    dbname = db_config.get('dbname')
    user = db_config.get('user')
    password = db_config.get('password')
    host = db_config.get('host')
    port = db_config.get('port')
    try:
        # PostgreSQL 기본 데이터베이스인 "postgres"에 접속합니다.
        conn = psycopg2.connect(dbname="postgres", user=user, password=password, host=host, port=port)
        conn.autocommit = True  # 자동 커밋 모드를 활성화합니다.
        cur = conn.cursor()
        # 타겟 데이터베이스의 존재 여부를 확인합니다.
        cur.execute("SELECT 1 FROM pg_database WHERE datname = %s", (dbname,))
        exists = cur.fetchone()
        if not exists:
            logger.debug(f"Database '{dbname}' does not exist. Creating database.")
            # 안전한 SQL 식별자 처리를 위해 psycopg2.sql 모듈 사용
            cur.execute(sql.SQL("CREATE DATABASE {}").format(sql.Identifier(dbname)))
        else:
            logger.debug(f"Database '{dbname}' already exists.")
        cur.close()
        conn.close()
    except Exception as e:
        logger.error(f"Error checking/creating database: {e}", exc_info=True)
        sys.exit(1)

def run_update_ohlcv_data():
    """
    최신 OHLCV 데이터를 가져와서 데이터베이스에 업데이트하는 메인 함수입니다.
    
    주요 단계:
      1. 데이터베이스가 존재하지 않으면 생성
      2. 상위 거래량 심볼(예: Binance에서 USDT 마켓 상위 심볼)과 각 심볼의 첫 거래 날짜를 조회
      3. 공통 시작 날짜(모든 심볼에 적용)를 결정
      4. 각 심볼과 각 시간 프레임(1d, 4h, 1h, 15m)에 대해:
            - 기존 데이터를 조회하고,
            - 데이터가 있다면 마지막 타임스탬프 이후부터, 없으면 공통 시작 날짜부터
            - 새로운 OHLCV 데이터를 API로 조회 후,
            - 조회된 데이터가 있으면 데이터베이스에 삽입
      5. 각 단계에서 에러 발생 시 로깅 처리 후 다음 처리로 진행 또는 종료
      
    Parameters:
        없음
    
    Returns:
        None
    """
    # 데이터베이스가 존재하지 않으면 생성합니다.
    create_database_if_not_exists(DATABASE)
    
    # 상위 거래량 심볼 조회: (심볼, 첫 거래일) 튜플 리스트 반환
    symbols_with_onboard = get_top_volume_symbols(exchange_id='binance', quote_currency='USDT', count=3)
    if not symbols_with_onboard:
        logger.error("No valid symbols found from Binance.")
        sys.exit(1)
    logger.debug(f"Top symbols (with onboard date): {symbols_with_onboard}")
    
    # 모든 심볼에 대해 공통 시작 날짜를 결정 (가장 늦은 온보딩 날짜 사용)
    global_start_date = get_latest_onboard_date(symbols_with_onboard, exchange_id='binance')
    logger.debug(f"Unified start date for all symbols: {global_start_date}")
    
    # 심볼 이름만 추출하여 리스트 생성
    symbols = [item[0] for item in symbols_with_onboard]
    # 데이터 업데이트할 다양한 시간 프레임 설정
    timeframes = ["1d", "4h", "1h", "15m"]
    
    # 업데이트 종료 시점을 현재 UTC 시간으로 설정
    end_date = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
    
    # 각 심볼과 각 시간 프레임에 대해 데이터 업데이트를 수행합니다.
    for symbol in symbols:
        symbol_key = symbol.replace("/", "").lower()  # 테이블명 생성을 위한 심볼 변환
        logger.debug(f"Processing {symbol} (table prefix: ohlcv_{symbol_key}_)")
        for tf in timeframes:
            table_name = f"ohlcv_{symbol_key}_{tf}"
            logger.debug(f"Processing {symbol} - {tf} (table: {table_name})")
            
            try:
                # 데이터베이스에서 기존 OHLCV 데이터를 조회합니다.
                df_existing = fetch_ohlcv_records(table_name=table_name)
            except Exception as e:
                logger.error(f"Error fetching existing data for table {table_name}: {e}", exc_info=True)
                df_existing = pd.DataFrame()  # 에러 발생 시 빈 DataFrame 사용
            
            if not df_existing.empty:
                # 기존 데이터가 있다면 마지막 타임스탬프 이후부터 새로운 데이터 조회
                last_timestamp = df_existing.index.max()
                new_start_dt = last_timestamp + timedelta(seconds=1)
                new_start_date = new_start_dt.strftime("%Y-%m-%d %H:%M:%S")
                logger.debug(f"Existing data found in {table_name}. Fetching new data from {new_start_date} to {end_date}.")
            else:
                # 데이터가 없으면 공통 시작 날짜부터 조회
                new_start_date = global_start_date
                logger.debug(f"No existing data in {table_name}. Fetching data from {new_start_date} to {end_date}.")
            
            try:
                # 지정된 기간 동안 OHLCV 데이터를 조회합니다.
                df_new = fetch_historical_ohlcv_data(
                    symbol=symbol,
                    timeframe=tf,
                    start_date=new_start_date,
                    exchange_id='binance'
                )
                if df_new.empty:
                    logger.debug(f"No new data fetched for {symbol} - {tf}.")
                    continue
                else:
                    logger.debug(f"Fetched {len(df_new)} new rows for {symbol} - {tf}.")
            except Exception as e:
                logger.error(f"Error fetching OHLCV data for {symbol} - {tf}: {e}", exc_info=True)
                continue
            
            try:
                # 조회된 새로운 데이터를 데이터베이스 테이블에 삽입합니다.
                insert_ohlcv_records(df_new, table_name=table_name)
                logger.debug(f"Inserted new data into table {table_name}.")
            except Exception as e:
                logger.error(f"Error inserting data into table {table_name}: {e}", exc_info=True)

if __name__ == "__main__":
    run_update_ohlcv_data()


---

# scripts/run_final_report.py

---

# scripts/run_market_analysis.py

---

# scripts/run_signal_calc.py

---

# scripts/run_trading.py
"""
이 스크립트는 전체 거래 전략의 성능 평가를 위한 프로젝트 테스트를 수행합니다.
주요 작업:
  1. 워크포워드 방식의 파라미터 최적화 수행
  2. 최적의 파라미터를 적용하여 각 자산에 대해 백테스팅 실행
  3. 백테스트 결과를 기반으로 거래 성과 계산 및 최종 보고서 생성
"""

from logs.log_config import setup_logger, initialize_root_logger, shutdown_logging  # 로깅 설정 함수들
from logs.logging_util import LoggingUtil  # 기존 로깅 파일 관리 함수
from strategies.optimizer import DynamicParameterOptimizer  # 동적 파라미터 최적화 클래스
from backtesting.backtester import Backtester  # 백테스팅 수행 클래스
from backtesting.performance import compute_performance  # 거래 성과 계산 함수
from reports.final_report import generate_final_report  # 최종 보고서 생성 함수
from parameters_sensitivity.config_manager import ConfigManager  # 설정 관리 클래스
from data.db.db_manager import get_unique_symbol_list, get_date_range  # DB 관련 함수
from data.db.db_config import DATABASE  # DB 접속 정보

def get_default_date_range(symbol: str, timeframe: str = "1d") -> tuple:
    """
    주어진 심볼과 시간 프레임에 대해 데이터베이스에서 날짜 범위를 조회합니다.
    
    Parameters:
        symbol (str): 거래 심볼 (예: "BTC/USDT")
        timeframe (str): 데이터의 시간 간격 (기본값: "1d")
    
    Returns:
        tuple: (시작 날짜, 종료 날짜) 문자열. 데이터가 없으면 기본 날짜 범위를 반환합니다.
    """
    symbol_key = symbol.replace("/", "").lower()  # 테이블명 생성을 위해 심볼 포맷 변환
    table_name = f"ohlcv_{symbol_key}_{timeframe}"
    start_date, end_date = get_date_range(table_name, DATABASE)
    if start_date is None or end_date is None:
        start_date, end_date = "2018-01-01 00:00:00", "2025-12-31 23:59:59"
    return start_date, end_date

def run_strategy_performance():
    """
    거래 전략의 성능을 평가하기 위한 전체 테스트를 수행합니다.
    
    주요 단계:
      1. 기존 로깅 파일 정리 및 로깅 시스템 초기화
      2. 워크포워드 파라미터 최적화를 통해 최적의 파라미터 탐색
      3. 각 자산별 백테스팅 실행 및 데이터 로드
      4. 백테스트 결과를 이용해 거래 성과 계산 후 최종 보고서 생성
      5. 로깅 종료
      
    Parameters:
        없음
    
    Returns:
        None
    """
    # 기존 로깅 파일 삭제 및 로깅 시스템 초기화
    LoggingUtil.clear_log_files()
    initialize_root_logger()

    logger = setup_logger(__name__)
    logger.info("Starting full project test.")

    # DB에서 분석 대상 자산 목록 조회, 없으면 기본 자산 리스트 사용
    assets = get_unique_symbol_list() or ["BTC/USDT", "ETH/USDT", "XRP/USDT"]
    logger.info(f"Assets for strategy performance: {assets}")

    logger.info("Starting Walk-Forward parameter optimization.")
    # 동적 파라미터 최적화를 위한 객체 생성 (최적화 반복 횟수: 10회)
    optimizer = DynamicParameterOptimizer(n_trials=10, assets=assets)
    best_trial = optimizer.optimize()  # 최적의 파라미터 탐색

    config_manager = ConfigManager()
    # 최적화된 파라미터와 기존 설정을 병합하여 최종 파라미터 결정
    best_params = config_manager.merge_optimized(best_trial.params)
    logger.info(f"Optimal parameters determined: {best_params}")

    # 대표 자산의 1일(1d) 데이터 테이블을 통해 기본 날짜 범위를 조회
    default_start, default_end = get_default_date_range(assets[0], "1d")
    logger.info(f"Using date range from DB: {default_start} to {default_end}")
    timeframes = {"short_tf": "4h", "long_tf": "1d"}

    # 각 자산에 대해 백테스팅 실행
    for symbol in assets:
        symbol_key = symbol.replace("/", "").lower()  # 테이블명 생성을 위해 심볼 형식 변환
        # 초기 계좌 크기를 10,000으로 설정하여 백테스터 인스턴스 생성
        backtester = Backtester(symbol=symbol, account_size=10000)
        try:
            # 단기 및 장기 데이터 테이블 형식 지정 후 데이터 로드
            backtester.load_data(
                short_table_format=f"ohlcv_{symbol_key}_{{timeframe}}",
                long_table_format=f"ohlcv_{symbol_key}_{{timeframe}}",
                short_tf=timeframes["short_tf"],
                long_tf=timeframes["long_tf"],
                start_date=default_start,
                end_date=default_end,
                use_weekly=True  # 주간 데이터도 함께 로드
            )
        except Exception as e:
            logger.error(f"Data load failed for {symbol}: {e}", exc_info=True)
            continue

        try:
            # 최적 파라미터를 적용하여 백테스트 실행; 실행 결과로 거래 내역 반환
            trades, _ = backtester.run_backtest(dynamic_params=best_params)
            logger.info(f"Backtest complete for {symbol}: {len(trades)} trades executed.")
        except Exception as e:
            logger.error(f"Backtest error for {symbol}: {e}", exc_info=True)
            continue

        if trades:
            # 거래 성과 계산 (예: 수익률, 승률 등) 후 최종 보고서 생성
            performance_data = compute_performance(trades, weekly_data=backtester.df_weekly)
            generate_final_report(performance_data, symbol=symbol)
        else:
            logger.info(f"No trades executed for {symbol}.")

    logger.info("Project test complete.")
    shutdown_logging()  # 로깅 시스템 종료

if __name__ == "__main__":
    run_strategy_performance()
