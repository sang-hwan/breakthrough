[logs/aggregating_handler.py]
# logs/aggregating_handler.py
import logging
import os
import threading

class AggregatingHandler(logging.Handler):
    """
    AggregatingHandler는 (logger 이름, 파일명, 함수명)별로 이벤트 발생 횟수를 집계합니다.
    플러시 시 집계된 결과를 최종 로그로 출력하고, 집계를 초기화합니다.
    """

    def __init__(self, level=logging.DEBUG):
        super().__init__(level)
        self.total_aggregation = {}
        self.weekly_signal_aggregation = {}
        self.lock = threading.RLock()

    def emit(self, record):
        try:
            key = (record.name, os.path.basename(record.pathname), record.funcName)
            with self.lock:
                self.total_aggregation[key] = self.total_aggregation.get(key, 0) + 1
                if getattr(record, 'is_weekly_signal', False):
                    self.weekly_signal_aggregation[key] = self.weekly_signal_aggregation.get(key, 0) + 1
        except Exception:
            self.handleError(record)

    def flush_aggregation_summary(self):
        with self.lock:
            if self.total_aggregation:
                summary_lines = [
                    f"{filename}:{funcname} (logger: {logger_name}) - 총 {count}회 발생"
                    for (logger_name, filename, funcname), count in self.total_aggregation.items()
                ]
                summary = "\n".join(summary_lines)
                try:
                    logging.getLogger().info("전체 누적 로그 집계:\n" + summary)
                except Exception:
                    pass
                self.total_aggregation.clear()
            if self.weekly_signal_aggregation:
                weekly_summary_lines = [
                    f"{filename}:{funcname} (logger: {logger_name}) - 주간 신호 {count}회 발생"
                    for (logger_name, filename, funcname), count in self.weekly_signal_aggregation.items()
                ]
                weekly_summary = "\n".join(weekly_summary_lines)
                try:
                    logging.getLogger().info("전체 주간 신호 로그 집계:\n" + weekly_summary)
                except Exception:
                    pass
                self.weekly_signal_aggregation.clear()

[logs/final_report.py]
# logs/final_report.py
from logs.logger_config import setup_logger

logger = setup_logger(__name__)

def generate_final_report(performance_data, symbol=None):
    overall = performance_data.get("overall", {})
    report_lines = []
    header = f"=== FINAL BACKTEST PERFORMANCE REPORT for {symbol} ===" if symbol else "=== FINAL BACKTEST PERFORMANCE REPORT ==="
    report_lines.append(header)
    report_lines.append(f"Overall ROI: {overall.get('roi', 0):.2f}%")
    report_lines.append(f"Cumulative Return: {overall.get('cumulative_return', 0):.2f}")
    report_lines.append(f"Total PnL: {overall.get('total_pnl', 0):.2f}")
    report_lines.append(f"Trade Count: {overall.get('trade_count', 0)}")
    report_lines.append("")
    report_lines.append("Performance Overview:")
    report_lines.append(f"  Annualized Return: {overall.get('annualized_return', 0):.2f}%")
    report_lines.append(f"  Annualized Volatility: {overall.get('annualized_volatility', 0):.2f}%")
    report_lines.append(f"  Sharpe Ratio: {overall.get('sharpe_ratio', 0):.2f}")
    report_lines.append(f"  Sortino Ratio: {overall.get('sortino_ratio', 0):.2f}")
    report_lines.append(f"  Calmar Ratio: {overall.get('calmar_ratio', 0):.2f}")
    report_lines.append(f"  Maximum Drawdown: {overall.get('max_drawdown', 0):.2f}")
    report_lines.append("")
    report_lines.append("Weekly Strategy Metrics:")
    weekly = performance_data.get("weekly", {})
    report_lines.append(f"  Weekly ROI: {weekly.get('weekly_roi', 0):.2f}%")
    report_lines.append(f"  Weekly Max Drawdown: {weekly.get('weekly_max_drawdown', 0):.2f}%")
    report_lines.append("")
    report_lines.append("Trading Stats:")
    report_lines.append(f"  Win Rate: {overall.get('win_rate', 0):.2f}%")
    report_lines.append(f"  Average Win: {overall.get('avg_win', 0):.2f}")
    report_lines.append(f"  Average Loss: {overall.get('avg_loss', 0):.2f}")
    report_lines.append(f"  Profit Factor: {overall.get('profit_factor', 0):.2f}")
    report_lines.append(f"  Trades per Year: {overall.get('trades_per_year', 0):.2f}")
    report_lines.append(f"  Max Consecutive Wins: {overall.get('max_consecutive_wins', 0)}")
    report_lines.append(f"  Max Consecutive Losses: {overall.get('max_consecutive_losses', 0)}")
    report_lines.append("")
    report_lines.append("Monthly Performance:")
    monthly = performance_data.get("monthly", {})
    for month in sorted(monthly.keys()):
        data = monthly[month]
        status = "TARGET MET" if data["roi"] >= 2.0 else "TARGET NOT MET"
        report_lines.append(f"  {month}: ROI {data['roi']:.2f}% (Trades: {data['trade_count']}) --> {status}")
    report_lines.append("=========================================")
    
    report_str = "\n".join(report_lines)
    logger.info(report_str)

def generate_parameter_sensitivity_report(param_name, results):
    report_lines = []
    report_lines.append("=== FINAL PARAMETER SENSITIVITY REPORT ===")
    
    if all(isinstance(k, tuple) for k in results.keys()):
        report_lines.append("Multi-Parameter Analysis Results:")
        for combo_key, metrics in results.items():
            combo_str = ", ".join([f"{p}={v:.4f}" for p, v in combo_key])
            report_lines.append(f"Combination: {combo_str}")
            if metrics is not None:
                for metric_name, stats in metrics.items():
                    report_lines.append(f"  {metric_name}: mean={stats['mean']:.2f}, std={stats['std']:.2f}, min={stats['min']:.2f}, max={stats['max']:.2f}")
            else:
                report_lines.append("  Error during backtesting for this combination.")
            report_lines.append("")
    else:
        report_lines.append(f"Analyzed Parameter: {param_name}")
        report_lines.append("Results:")
        for val in sorted(results.keys()):
            result = results[val]
            if result is not None:
                roi = result.get("roi", 0)
                report_lines.append(f"{param_name} = {val:.4f} -> ROI: {roi:.2f}%")
            else:
                report_lines.append(f"{param_name} = {val:.4f} -> ROI: Error")
    report_lines.append("==========================================")
    
    report_str = "\n".join(report_lines)
    logger.info(report_str)

def generate_weekly_signal_report(weekly_signal_counts):
    report_lines = []
    report_lines.append("=== WEEKLY SIGNAL REPORT ===")
    for (logger_name, filename, funcname), count in weekly_signal_counts.items():
        report_lines.append(f"{filename}:{funcname} (logger: {logger_name}) - 주간 신호 {count}회 발생")
    report_lines.append("==========================================")
    
    report_str = "\n".join(report_lines)
    logger.info(report_str)

[logs/logger_config.py]
# logs/logger_config.py
import logging
import os
import queue
from logging.handlers import RotatingFileHandler, QueueHandler, QueueListener
from logs.aggregating_handler import AggregatingHandler

FILE_LOG_LEVEL = logging.INFO         # 파일 로그는 INFO 이상
detail_level = logging.DEBUG           # 콘솔 로그는 DEBUG 이상

LOG_FILES_DIR = os.path.join("logs", "log_files")
if not os.path.exists(LOG_FILES_DIR):
    os.makedirs(LOG_FILES_DIR)
BASE_LOG_FILE = os.path.join(LOG_FILES_DIR, "project.log")

class OneLineFormatter(logging.Formatter):
    def format(self, record):
        formatted = super().format(record)
        return formatted.replace("\n", " | ")

class LineRotatingFileHandler(RotatingFileHandler):
    def __init__(self, base_filename, mode='a', max_lines=500, encoding=None, delay=False):
        self.base_filename = base_filename
        self.current_index = 0
        self._set_current_filename()
        super().__init__(self.current_filename, mode, maxBytes=0, encoding=encoding, delay=delay)
        self.max_lines = max_lines
        self.current_line_count = 0

    def _set_current_filename(self):
        base, ext = os.path.splitext(self.base_filename)
        self.current_filename = self.base_filename if self.current_index == 0 else f"{base}{self.current_index}{ext}"
        self.baseFilename = os.path.abspath(self.current_filename)

    def doRollover(self):
        if self.stream:
            self.stream.close()
            self.stream = None
        self.current_index += 1
        self._set_current_filename()
        self.mode = 'w'
        self.stream = self._open()
        self.current_line_count = 0

    def emit(self, record):
        try:
            if record.levelno < FILE_LOG_LEVEL:
                return
            msg = self.format(record)
            lines_in_msg = msg.count("\n") or 1
            if self.current_line_count + lines_in_msg > self.max_lines:
                self.doRollover()
            self.current_line_count += lines_in_msg
            super().emit(record)
        except Exception:
            self.handleError(record)

log_queue = queue.Queue(-1)
queue_listener = None

def initialize_root_logger():
    root_logger = logging.getLogger()
    root_logger.setLevel(detail_level)

    # 기존 핸들러 제거
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # 파일 핸들러 (로그 파일은 log_files 디렉토리에 생성됨)
    file_handler = LineRotatingFileHandler(
        base_filename=BASE_LOG_FILE,
        max_lines=500,
        encoding="utf-8",
        delay=True
    )
    file_handler.setLevel(FILE_LOG_LEVEL)
    formatter = OneLineFormatter('[%(asctime)s] %(levelname)s:%(name)s:%(filename)s:%(funcName)s: %(message)s')
    file_handler.setFormatter(formatter)
    root_logger.addHandler(file_handler)

    # 콘솔 핸들러
    console_handler = logging.StreamHandler()
    console_handler.setLevel(detail_level)
    console_formatter = OneLineFormatter('[%(asctime)s] %(levelname)s:%(name)s:%(filename)s:%(funcName)s: %(message)s')
    console_handler.setFormatter(console_formatter)
    console_handler.addFilter(lambda record: not getattr(record, '_is_summary', False))

    q_handler = QueueHandler(log_queue)
    root_logger.addHandler(q_handler)

    global queue_listener
    queue_listener = QueueListener(log_queue, console_handler)
    queue_listener.start()

    # AggregatingHandler 추가
    try:
        aggregator_handler = AggregatingHandler(level=detail_level)
        aggregator_handler.addFilter(lambda record: not getattr(record, '_is_summary', False))
        aggregator_formatter = OneLineFormatter('[%(asctime)s] %(levelname)s:%(name)s:%(filename)s:%(funcName)s: %(message)s')
        aggregator_handler.setFormatter(aggregator_formatter)
        root_logger.addHandler(aggregator_handler)
    except Exception as e:
        logging.getLogger().error("Failed to add AggregatingHandler: " + str(e), exc_info=True)

    # 외부 라이브러리 로그 레벨 조정
    logging.getLogger("ccxt").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)

def setup_logger(module_name: str) -> logging.Logger:
    logger = logging.getLogger(module_name)
    logger.setLevel(detail_level)
    logger.propagate = True
    try:
        agg_handler = AggregatingHandler(level=detail_level)
        agg_handler.addFilter(lambda record: not getattr(record, '_is_summary', False))
        formatter = OneLineFormatter('[%(asctime)s] %(levelname)s:%(name)s:%(filename)s:%(funcName)s: %(message)s')
        agg_handler.setFormatter(formatter)
        logger.addHandler(agg_handler)
    except Exception as e:
        logger.error("Failed to add module-specific AggregatingHandler: " + str(e), exc_info=True)
    return logger

def shutdown_logging():
    root_logger = logging.getLogger()
    for handler in root_logger.handlers:
        try:
            if hasattr(handler, 'flush_aggregation_summary'):
                handler.flush_aggregation_summary()
        except Exception:
            pass
    global queue_listener
    if queue_listener is not None:
        queue_listener.stop()
        queue_listener = None
    logging.shutdown()

[logs/logging_util.py]
# logs/logging_util.py
import threading
import time
from logs.logger_config import setup_logger, LOG_FILES_DIR
from logs.state_change_manager import StateChangeManager

class DynamicLogTracker:
    """
    각 state_key별로 최근 이벤트 발생 빈도를 EMA(지수이동평균)로 계산하여 저장합니다.
    """
    def __init__(self, alpha=0.1, baseline=1.0):
        self.alpha = alpha
        self.baseline = baseline
        self.data = {}  # state_key -> {'last_time': timestamp, 'ema_freq': float}

    def update(self, state_key, current_time):
        if state_key not in self.data:
            self.data[state_key] = {'last_time': current_time, 'ema_freq': 0.0}
            return 0.0
        else:
            last_time = self.data[state_key]['last_time']
            dt = current_time - last_time
            freq = 1.0 / dt if dt > 0 else 100.0
            ema_old = self.data[state_key]['ema_freq']
            ema_new = self.alpha * freq + (1 - self.alpha) * ema_old
            self.data[state_key]['ema_freq'] = ema_new
            self.data[state_key]['last_time'] = current_time
            return ema_new

    def get_ema(self, state_key):
        return self.data.get(state_key, {}).get('ema_freq', 0.0)

class LoggingUtil:
    """
    이벤트 로그를 기록할 때 동적 필터링을 적용합니다.
    동일 이벤트(state_key)의 중복 기록을 방지하고, EMA 기반 필터링을 통해
    중요도에 따라 로그 레벨을 조절하거나 생략합니다.
    """
    def __init__(self, module_name: str):
        self.module_name = module_name
        self.lock = threading.RLock()
        self.logger = setup_logger(module_name)
        self.state_manager = StateChangeManager()
        self.log_tracker = DynamicLogTracker(alpha=0.1, baseline=1.0)
    
    def log_event(self, event_message: str, state_key: str = None, importance: str = 'MEDIUM'):
        with self.lock:
            current_time = time.time()
            ema = 0.0
            if state_key:
                ema = self.log_tracker.update(state_key, current_time)
            
            if state_key and not self.state_manager.has_changed(state_key, event_message):
                return
            
            effective_level = 'INFO'
            if ema > self.log_tracker.baseline * 2:
                if importance.upper() == 'LOW':
                    return  # LOW 중요도 이벤트는 생략
                elif importance.upper() == 'MEDIUM':
                    effective_level = 'DEBUG'
                else:
                    effective_level = 'INFO'
            
            msg = f"[{self.module_name}] Event: {event_message}"
            if effective_level == 'DEBUG':
                self.logger.debug(msg)
            else:
                self.logger.info(msg)
    
    @staticmethod
    def clear_log_files():
        import os, glob
        log_files = glob.glob(os.path.join(LOG_FILES_DIR, "*.log"))
        for log_file in log_files:
            try:
                os.remove(log_file)
                print(f"Deleted log file: {log_file}")
            except Exception as e:
                print(f"Failed to remove log file {log_file}: {e}")

[logs/state_change_manager.py]
# logs/state_change_manager.py
class StateChangeManager:
    def __init__(self, numeric_threshold: float = 0.01):
        """
        :param numeric_threshold: 숫자형 상태 값의 상대 변화가 이 값 이상일 때만 변화를 감지 (기본 1%).
        """
        self._state_dict = {}
        self.numeric_threshold = numeric_threshold

    def has_changed(self, key: str, new_value) -> bool:
        """
        key에 해당하는 상태가 이전과 비교하여 의미 있는 변화(숫자형이면 상대 1% 이상, 그 외는 단순 불일치)가 있으면 True를 반환하고, 새로운 값을 저장합니다.
        """
        old_value = self._state_dict.get(key)
        # 처음 상태인 경우
        if old_value is None:
            self._state_dict[key] = new_value
            return True

        # 숫자형 값인 경우: 상대 변화율 비교 (단, old_value가 0이면 절대 변화량으로 판단)
        if isinstance(old_value, (int, float)) and isinstance(new_value, (int, float)):
            if old_value == 0:
                # 0인 경우, 절대 변화량이 임계값보다 크면 변화로 판단
                if abs(new_value) >= self.numeric_threshold:
                    self._state_dict[key] = new_value
                    return True
                else:
                    return False
            else:
                relative_change = abs(new_value - old_value) / abs(old_value)
                if relative_change >= self.numeric_threshold:
                    self._state_dict[key] = new_value
                    return True
                else:
                    return False
        else:
            # 숫자형이 아닌 경우에는 기존 방식 그대로
            if old_value != new_value:
                self._state_dict[key] = new_value
                return True
            else:
                return False

    def get_state(self, key: str):
        """
        현재 저장된 상태 값을 반환합니다.
        """
        return self._state_dict.get(key)

    def reset_state(self, key: str = None):
        """
        상태를 리셋합니다.
        - key가 제공되면 해당 key의 상태만 리셋.
        - key가 없으면 전체 상태를 초기화.
        """
        if key:
            if key in self._state_dict:
                del self._state_dict[key]
        else:
            self._state_dict.clear()
