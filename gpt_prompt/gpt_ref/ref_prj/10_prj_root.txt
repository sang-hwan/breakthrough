[requirements.txt]
pandas
numpy
SQLAlchemy
psycopg2-binary
optuna
ta
ccxt
hmmlearn
python-dotenv
pytest
pydantic

[run_drop_db_tables.py]
# run_drop_db_tables.py
import sys
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from data.db.db_config import DATABASE
from logs.logger_config import initialize_root_logger, setup_logger

# 환경변수 로드 및 로깅 초기화
load_dotenv()
initialize_root_logger()
logger = setup_logger(__name__)

def drop_all_tables(db_config):
    try:
        engine = create_engine(
            f"postgresql://{db_config['user']}:{db_config['password']}@"
            f"{db_config['host']}:{db_config['port']}/{db_config['dbname']}",
            pool_pre_ping=True
        )
        with engine.begin() as conn:
            # public 스키마 내 테이블 목록 조회
            result = conn.execute(text("SELECT tablename FROM pg_tables WHERE schemaname = 'public'"))
            tables = [row[0] for row in result]
            if not tables:
                logger.debug("No tables found in the database.")
                return
            for table in tables:
                logger.debug(f"Dropping table {table}...")
                conn.execute(text(f"DROP TABLE IF EXISTS {table} CASCADE"))
            logger.debug("All tables dropped successfully.")
    except Exception as e:
        logger.error(f"Error dropping tables: {e}", exc_info=True)
        sys.exit(1)

def run_drop_db_tables():
    drop_all_tables(DATABASE)

if __name__ == "__main__":
    run_drop_db_tables()

[run_parameter_analysis.py]
# run_parameter_analysis.py
import argparse
import logging
import numpy as np
from logs.logger_config import setup_logger, initialize_root_logger, shutdown_logging
from logs.logging_util import LoggingUtil
from strategies.param_analysis import run_sensitivity_analysis
from logs.final_report import generate_parameter_sensitivity_report
from data.db.db_manager import get_unique_symbol_list, get_date_range
from data.db.db_config import DATABASE

def parse_args():
    parser = argparse.ArgumentParser(
        description="Run parameter sensitivity analysis for trading strategies."
    )
    parser.add_argument("--param_names", type=str, 
                        default="profit_ratio,atr_multiplier,risk_per_trade,scale_in_threshold,weekly_breakout_threshold,weekly_momentum_threshold",
                        help="Comma-separated list of parameter names to analyze.")
    parser.add_argument("--param_steps", type=int, default=3, 
                        help="Number of steps for each parameter (default: 3)")
    parser.add_argument("--assets", type=str, default="",
                        help="Optional: Comma-separated list of assets. If not provided, DB will be queried for unique symbols.")
    parser.add_argument("--short_tf", type=str, default="4h", 
                        help="Short time frame (default: 4h)")
    parser.add_argument("--long_tf", type=str, default="1d", 
                        help="Long time frame (default: 1d)")
    parser.add_argument("--periods", type=str, default="", 
                        help="Optional multiple periods in format start1:end1;start2:end2, etc.")
    # 새 인자: 분석 기간 축소를 위한 인자 (예: "2022-01-01:2023-01-01")
    parser.add_argument("--analysis_period", type=str, default="2022-01-01:2023-01-01",
                        help="Analysis period in format start_date:end_date to reduce overall analysis period.")
    return parser.parse_args()

def parse_assets(asset_str):
    return [asset.strip() for asset in asset_str.split(",") if asset.strip()]

def parse_periods(periods_str, default_start, default_end):
    if not periods_str:
        return [(default_start, default_end)]
    period_list = []
    for pair in periods_str.split(";"):
        if pair:
            try:
                s, e = pair.split(":")
                period_list.append((s.strip(), e.strip()))
            except Exception as e:
                logging.getLogger(__name__).error(f"Error parsing period pair '{pair}': {e}", exc_info=True)
    return period_list if period_list else [(default_start, default_end)]

def get_default_date_range(symbol: str, timeframe: str = "1d") -> tuple:
    symbol_key = symbol.replace("/", "").lower()
    table_name = f"ohlcv_{symbol_key}_{timeframe}"
    start_date, end_date = get_date_range(table_name, DATABASE)
    # analysis_period 인자를 사용하므로 기본값을 무시합니다.
    if start_date is None or end_date is None:
        start_date, end_date = "2022-01-01 00:00:00", "2023-01-01 23:59:59"
    return start_date, end_date

def run_parameter_analysis():
    LoggingUtil.clear_log_files()
    initialize_root_logger()

    args = parse_args()
    logger = setup_logger(__name__)
    logger.info("Starting parameter sensitivity analysis.")

    if args.assets:
        assets = parse_assets(args.assets)
    else:
        assets = get_unique_symbol_list() or ["BTC/USDT"]
    logger.info(f"Assets for analysis: {assets}")

    # 분석 기간 인자 파싱 (예: "2022-01-01:2023-01-01")
    try:
        analysis_start, analysis_end = args.analysis_period.split(":")
        analysis_start = analysis_start.strip() + " 00:00:00"
        analysis_end = analysis_end.strip() + " 23:59:59"
    except Exception as e:
        logger.error(f"Error parsing analysis_period: {e}", exc_info=True)
        analysis_start, analysis_end = "2022-01-01 00:00:00", "2023-01-01 23:59:59"
    logger.info(f"Using analysis period: {analysis_start} to {analysis_end}")

    # DB에서 날짜 범위를 조회할 필요 없이, analysis_period을 기본 날짜 범위로 사용
    default_start, default_end = analysis_start, analysis_end
    logger.info(f"Default analysis date range: {default_start} to {default_end}")
    periods = parse_periods(args.periods, default_start, default_end)

    from config.config_manager import ConfigManager
    cm = ConfigManager()
    defaults = cm.get_defaults()
    param_names = [p.strip() for p in args.param_names.split(",") if p.strip()]
    param_settings = {}
    for pname in param_names:
        if pname not in defaults:
            logger.warning(f"Parameter {pname} not found in default parameters. Skipping.")
            continue
        try:
            default_val = float(defaults[pname])
        except Exception:
            logger.warning(f"Parameter {pname} is not numeric. Skipping.")
            continue
        start_val = default_val * 0.9
        end_val = default_val * 1.1
        param_settings[pname] = np.linspace(start_val, end_val, args.param_steps)
        logger.info(f"Analyzing {pname} over range {start_val:.4f} to {end_val:.4f}")

    results_all = run_sensitivity_analysis(
        param_settings, assets, args.short_tf, args.long_tf, default_start, default_end, periods
    )
    report_title = "Multi-Parameter Analysis: " + ", ".join([str(k) for k in results_all.keys()])
    generate_parameter_sensitivity_report(report_title, results_all)
    shutdown_logging()

if __name__ == "__main__":
    run_parameter_analysis()

[run_strategy_performance.py]
# run_strategy_performance.py
from logs.logger_config import setup_logger, initialize_root_logger, shutdown_logging
from logs.logging_util import LoggingUtil
from strategies.optimizer import DynamicParameterOptimizer
from backtesting.backtester import Backtester
from backtesting.performance import compute_performance
from logs.final_report import generate_final_report
from config.config_manager import ConfigManager
from data.db.db_manager import get_unique_symbol_list, get_date_range
from data.db.db_config import DATABASE

def get_default_date_range(symbol: str, timeframe: str = "1d") -> tuple:
    symbol_key = symbol.replace("/", "").lower()
    table_name = f"ohlcv_{symbol_key}_{timeframe}"
    start_date, end_date = get_date_range(table_name, DATABASE)
    if start_date is None or end_date is None:
        start_date, end_date = "2018-01-01 00:00:00", "2025-12-31 23:59:59"
    return start_date, end_date

def run_strategy_performance():
    LoggingUtil.clear_log_files()
    initialize_root_logger()

    logger = setup_logger(__name__)
    logger.info("Starting full project test.")

    assets = get_unique_symbol_list() or ["BTC/USDT", "ETH/USDT", "XRP/USDT"]
    logger.info(f"Assets for strategy performance: {assets}")

    logger.info("Starting Walk-Forward parameter optimization.")
    optimizer = DynamicParameterOptimizer(n_trials=10, assets=assets)
    best_trial = optimizer.optimize()

    config_manager = ConfigManager()
    best_params = config_manager.merge_optimized(best_trial.params)
    logger.info(f"Optimal parameters determined: {best_params}")

    # DB에서 대표 심볼의 1d 테이블을 통해 날짜 범위를 조회
    default_start, default_end = get_default_date_range(assets[0], "1d")
    logger.info(f"Using date range from DB: {default_start} to {default_end}")
    timeframes = {"short_tf": "4h", "long_tf": "1d"}

    for symbol in assets:
        symbol_key = symbol.replace("/", "").lower()
        backtester = Backtester(symbol=symbol, account_size=10000)
        try:
            backtester.load_data(
                short_table_format=f"ohlcv_{symbol_key}_{{timeframe}}",
                long_table_format=f"ohlcv_{symbol_key}_{{timeframe}}",
                short_tf=timeframes["short_tf"],
                long_tf=timeframes["long_tf"],
                start_date=default_start,
                end_date=default_end,
                use_weekly=True
            )
        except Exception as e:
            logger.error(f"Data load failed for {symbol}: {e}", exc_info=True)
            continue

        try:
            trades, _ = backtester.run_backtest(dynamic_params=best_params)
            logger.info(f"Backtest complete for {symbol}: {len(trades)} trades executed.")
        except Exception as e:
            logger.error(f"Backtest error for {symbol}: {e}", exc_info=True)
            continue

        if trades:
            performance_data = compute_performance(trades, weekly_data=backtester.df_weekly)
            generate_final_report(performance_data, symbol=symbol)
        else:
            logger.info(f"No trades executed for {symbol}.")

    logger.info("Project test complete.")
    shutdown_logging()

if __name__ == "__main__":
    run_strategy_performance()

[run_update_ohlcv_data.py]
# run_update_ohlcv_data.py
import sys
from datetime import datetime, timedelta, timezone
import pandas as pd
from dotenv import load_dotenv
import psycopg2
from psycopg2 import sql

from data.db.db_config import DATABASE
from data.db.db_manager import fetch_ohlcv_records, insert_ohlcv_records
from data.ohlcv.ohlcv_fetcher import fetch_historical_ohlcv_data, get_top_volume_symbols, get_latest_onboard_date
from logs.logger_config import initialize_root_logger, setup_logger

load_dotenv()
initialize_root_logger()
logger = setup_logger(__name__)

def create_database_if_not_exists(db_config):
    """
    .env에 명시된 DB가 존재하지 않으면, postgres 기본 DB에 접속하여 해당 DB를 생성합니다.
    """
    dbname = db_config.get('dbname')
    user = db_config.get('user')
    password = db_config.get('password')
    host = db_config.get('host')
    port = db_config.get('port')
    try:
        conn = psycopg2.connect(dbname="postgres", user=user, password=password, host=host, port=port)
        conn.autocommit = True
        cur = conn.cursor()
        cur.execute("SELECT 1 FROM pg_database WHERE datname = %s", (dbname,))
        exists = cur.fetchone()
        if not exists:
            logger.debug(f"Database '{dbname}' does not exist. Creating database.")
            cur.execute(sql.SQL("CREATE DATABASE {}").format(sql.Identifier(dbname)))
        else:
            logger.debug(f"Database '{dbname}' already exists.")
        cur.close()
        conn.close()
    except Exception as e:
        logger.error(f"Error checking/creating database: {e}", exc_info=True)
        sys.exit(1)

def run_update_ohlcv_data():
    create_database_if_not_exists(DATABASE)
    
    # get_top_volume_symbols는 (symbol, first_available_date) 튜플 리스트를 반환합니다.
    symbols_with_onboard = get_top_volume_symbols(exchange_id='binance', quote_currency='USDT', count=3)
    if not symbols_with_onboard:
        logger.error("No valid symbols found from Binance.")
        sys.exit(1)
    logger.debug(f"Top symbols (with onboard date): {symbols_with_onboard}")
    
    # get_latest_onboard_date는 심볼 튜플을 그대로 받습니다.
    global_start_date = get_latest_onboard_date(symbols_with_onboard, exchange_id='binance')
    logger.debug(f"Unified start date for all symbols: {global_start_date}")
    
    # 테이블명 생성 등에서 사용할 심볼 이름만 추출합니다.
    symbols = [item[0] for item in symbols_with_onboard]
    timeframes = ["1d", "4h", "1h", "15m"]
    
    end_date = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
    
    for symbol in symbols:
        symbol_key = symbol.replace("/", "").lower()
        logger.debug(f"Processing {symbol} (table prefix: ohlcv_{symbol_key}_)")
        for tf in timeframes:
            table_name = f"ohlcv_{symbol_key}_{tf}"
            logger.debug(f"Processing {symbol} - {tf} (table: {table_name})")
            
            try:
                df_existing = fetch_ohlcv_records(table_name=table_name)
            except Exception as e:
                logger.error(f"Error fetching existing data for table {table_name}: {e}", exc_info=True)
                df_existing = pd.DataFrame()
            
            if not df_existing.empty:
                last_timestamp = df_existing.index.max()
                new_start_dt = last_timestamp + timedelta(seconds=1)
                new_start_date = new_start_dt.strftime("%Y-%m-%d %H:%M:%S")
                logger.debug(f"Existing data found in {table_name}. Fetching new data from {new_start_date} to {end_date}.")
            else:
                new_start_date = global_start_date
                logger.debug(f"No existing data in {table_name}. Fetching data from {new_start_date} to {end_date}.")
            
            try:
                df_new = fetch_historical_ohlcv_data(
                    symbol=symbol,
                    timeframe=tf,
                    start_date=new_start_date,
                    exchange_id='binance'
                )
                if df_new.empty:
                    logger.debug(f"No new data fetched for {symbol} - {tf}.")
                    continue
                else:
                    logger.debug(f"Fetched {len(df_new)} new rows for {symbol} - {tf}.")
            except Exception as e:
                logger.error(f"Error fetching OHLCV data for {symbol} - {tf}: {e}", exc_info=True)
                continue
            
            try:
                insert_ohlcv_records(df_new, table_name=table_name)
                logger.debug(f"Inserted new data into table {table_name}.")
            except Exception as e:
                logger.error(f"Error inserting data into table {table_name}: {e}", exc_info=True)

if __name__ == "__main__":
    run_update_ohlcv_data()
