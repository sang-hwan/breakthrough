[logs module code]
# logs/aggregating_handler.py
import logging
import os
import threading

class AggregatingHandler(logging.Handler):
    """
    AggregatingHandler aggregates log occurrence counts by (logger name, filename, function name).
    Additionally, logs with the 'is_weekly_signal' flag are aggregated separately to track weekly strategy signal events.
    Upon flush, it outputs summaries of the total aggregated logs and the weekly signal logs.
    """
    def __init__(self, level=logging.DEBUG):
        super().__init__(level)
        self.total_aggregation = {}
        self.weekly_signal_aggregation = {}
        # 재진입(Reentrant) Lock을 사용하여 flush_aggregation_summary 중 재호출 시 데드락을 방지합니다.
        self.lock = threading.RLock()

    def emit(self, record):
        try:
            key = (record.name, os.path.basename(record.pathname), record.funcName)
            with self.lock:
                self.total_aggregation[key] = self.total_aggregation.get(key, 0) + 1

                # 만약 record에 'is_weekly_signal' 플래그가 있으면 별도로 집계
                if getattr(record, 'is_weekly_signal', False):
                    self.weekly_signal_aggregation[key] = self.weekly_signal_aggregation.get(key, 0) + 1
        except Exception:
            self.handleError(record)

    def flush_aggregation_summary(self):
        with self.lock:
            # 전체 누적 로그 집계 출력
            if self.total_aggregation:
                summary_lines = [
                    f"{filename}:{funcname} (logger: {logger_name}) - 총 {count}회 발생"
                    for (logger_name, filename, funcname), count in self.total_aggregation.items()
                ]
                summary = "\n".join(summary_lines)
                try:
                    logging.getLogger().info("전체 누적 로그 집계:\n" + summary)
                except Exception:
                    pass

            # 주간 신호 로그 집계 출력 (존재하는 경우)
            if self.weekly_signal_aggregation:
                weekly_summary_lines = [
                    f"{filename}:{funcname} (logger: {logger_name}) - 주간 신호 {count}회 발생"
                    for (logger_name, filename, funcname), count in self.weekly_signal_aggregation.items()
                ]
                weekly_summary = "\n".join(weekly_summary_lines)
                try:
                    logging.getLogger().info("전체 주간 신호 로그 집계:\n" + weekly_summary)
                except Exception:
                    pass

---

# logs/final_report.py
from logs.logger_config import setup_logger

logger = setup_logger(__name__)

def generate_final_report(performance_data, symbol=None):
    """
    종목별 백테스트 최종 성과 리포트를 생성합니다.
    (symbol이 전달되면 헤더에 포함)
    """
    overall = performance_data.get("overall", {})
    report_lines = []
    header = f"=== FINAL BACKTEST PERFORMANCE REPORT for {symbol} ===" if symbol else "=== FINAL BACKTEST PERFORMANCE REPORT ==="
    report_lines.append(header)
    report_lines.append(f"Overall ROI: {overall.get('roi', 0):.2f}%")
    report_lines.append(f"Cumulative Return: {overall.get('cumulative_return', 0):.2f}")
    report_lines.append(f"Total PnL: {overall.get('total_pnl', 0):.2f}")
    report_lines.append(f"Trade Count: {overall.get('trade_count', 0)}")
    report_lines.append("")
    report_lines.append("Performance Overview:")
    report_lines.append(f"  Annualized Return: {overall.get('annualized_return', 0):.2f}%")
    report_lines.append(f"  Annualized Volatility: {overall.get('annualized_volatility', 0):.2f}%")
    report_lines.append(f"  Sharpe Ratio: {overall.get('sharpe_ratio', 0):.2f}")
    report_lines.append(f"  Sortino Ratio: {overall.get('sortino_ratio', 0):.2f}")
    report_lines.append(f"  Calmar Ratio: {overall.get('calmar_ratio', 0):.2f}")
    report_lines.append(f"  Maximum Drawdown: {overall.get('max_drawdown', 0):.2f}")
    report_lines.append("")
    report_lines.append("Weekly Strategy Metrics:")
    weekly = performance_data.get("weekly", {})
    report_lines.append(f"  Weekly ROI: {weekly.get('weekly_roi', 0):.2f}%")
    report_lines.append(f"  Weekly Max Drawdown: {weekly.get('weekly_max_drawdown', 0):.2f}%")
    report_lines.append("")
    report_lines.append("Trading Stats:")
    report_lines.append(f"  Win Rate: {overall.get('win_rate', 0):.2f}%")
    report_lines.append(f"  Average Win: {overall.get('avg_win', 0):.2f}")
    report_lines.append(f"  Average Loss: {overall.get('avg_loss', 0):.2f}")
    report_lines.append(f"  Profit Factor: {overall.get('profit_factor', 0):.2f}")
    report_lines.append(f"  Trades per Year: {overall.get('trades_per_year', 0):.2f}")
    report_lines.append(f"  Max Consecutive Wins: {overall.get('max_consecutive_wins', 0)}")
    report_lines.append(f"  Max Consecutive Losses: {overall.get('max_consecutive_losses', 0)}")
    report_lines.append("")
    report_lines.append("Monthly Performance:")
    monthly = performance_data.get("monthly", {})
    for month in sorted(monthly.keys()):
        data = monthly[month]
        status = "TARGET MET" if data["roi"] >= 2.0 else "TARGET NOT MET"
        report_lines.append(f"  {month}: ROI {data['roi']:.2f}% (Trades: {data['trade_count']}) --> {status}")
    report_lines.append("=========================================")
    
    report_str = "\n".join(report_lines)
    logger.info(report_str)

def generate_parameter_sensitivity_report(param_name, results):
    """
    최종 파라미터 민감도 리포트를 생성합니다.
    (다중 파라미터 분석 시, 각 조합에 대한 평균/표준편차/최소/최대 지표를 출력)
    """
    report_lines = []
    report_lines.append("=== FINAL PARAMETER SENSITIVITY REPORT ===")
    
    if all(isinstance(k, tuple) for k in results.keys()):
        report_lines.append("Multi-Parameter Analysis Results:")
        for combo_key, metrics in results.items():
            combo_str = ", ".join([f"{p}={v:.4f}" for p, v in combo_key])
            report_lines.append(f"Combination: {combo_str}")
            if metrics is not None:
                for metric_name, stats in metrics.items():
                    report_lines.append(f"  {metric_name}: mean={stats['mean']:.2f}, std={stats['std']:.2f}, min={stats['min']:.2f}, max={stats['max']:.2f}")
            else:
                report_lines.append("  Error during backtesting for this combination.")
            report_lines.append("")
    else:
        report_lines.append(f"Analyzed Parameter: {param_name}")
        report_lines.append("Results:")
        for val in sorted(results.keys()):
            result = results[val]
            if result is not None:
                roi = result.get("roi", 0)
                report_lines.append(f"{param_name} = {val:.4f} -> ROI: {roi:.2f}%")
            else:
                report_lines.append(f"{param_name} = {val:.4f} -> ROI: Error")
    report_lines.append("==========================================")
    
    report_str = "\n".join(report_lines)
    logger.info(report_str)

def generate_weekly_signal_report(weekly_signal_counts):
    """
    주간 신호 발생 건수를 요약하는 리포트를 생성합니다.
    weekly_signal_counts는 (logger_name, filename, funcName)를 key로 하는 딕셔너리입니다.
    """
    report_lines = []
    report_lines.append("=== WEEKLY SIGNAL REPORT ===")
    for (logger_name, filename, funcname), count in weekly_signal_counts.items():
        report_lines.append(f"{filename}:{funcname} (logger: {logger_name}) - 주간 신호 {count}회 발생")
    report_lines.append("==========================================")
    
    report_str = "\n".join(report_lines)
    logger.info(report_str)

---

# logs/logger_config.py
import logging
import os
import queue
from logging.handlers import RotatingFileHandler, QueueHandler, QueueListener
from dotenv import load_dotenv
from logs.aggregating_handler import AggregatingHandler

load_dotenv()

ENVIRONMENT = os.getenv("ENVIRONMENT", "development").lower()
_LOG_LEVEL_FROM_ENV = os.getenv("LOG_LEVEL", None)
file_level = getattr(logging, _LOG_LEVEL_FROM_ENV.upper(), logging.INFO) if _LOG_LEVEL_FROM_ENV else logging.INFO
LOG_DETAIL_LEVEL = os.getenv("LOG_DETAIL_LEVEL", "DEBUG")
detail_level = getattr(logging, LOG_DETAIL_LEVEL.upper(), logging.DEBUG)
BASE_LOG_FILE = os.path.join("logs", "project.log")

class OneLineFormatter(logging.Formatter):
    def format(self, record):
        formatted = super().format(record)
        return formatted.replace("\n", " | ")

class LineRotatingFileHandler(RotatingFileHandler):
    def __init__(self, base_filename, mode='a', max_lines=1000, encoding=None, delay=False):
        self.base_filename = base_filename
        self.current_index = 0
        self._set_current_filename()
        super().__init__(self.current_filename, mode, maxBytes=0, encoding=encoding, delay=delay)
        self.max_lines = max_lines
        self.current_line_count = 0

    def _set_current_filename(self):
        base, ext = os.path.splitext(self.base_filename)
        self.current_filename = self.base_filename if self.current_index == 0 else f"{base}{self.current_index}{ext}"
        self.baseFilename = os.path.abspath(self.current_filename)

    def doRollover(self):
        if self.stream:
            self.stream.close()
            self.stream = None
        self.current_index += 1
        self._set_current_filename()
        self.mode = 'w'
        self.stream = self._open()
        self.current_line_count = 0

    def emit(self, record):
        try:
            msg = self.format(record)
            lines_in_msg = msg.count("\n") or 1
            if self.current_line_count + lines_in_msg > self.max_lines:
                self.doRollover()
            self.current_line_count += lines_in_msg
            super().emit(record)
        except Exception:
            self.handleError(record)

log_queue = queue.Queue(-1)
queue_listener = None

def initialize_root_logger():
    root_logger = logging.getLogger()
    root_logger.setLevel(detail_level)
    
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    file_handler = LineRotatingFileHandler(
        base_filename=BASE_LOG_FILE,
        max_lines=1000,
        encoding="utf-8",
        delay=True
    )
    file_handler.setLevel(file_level)
    formatter = OneLineFormatter('[%(asctime)s] %(levelname)s:%(name)s:%(filename)s:%(funcName)s: %(message)s')
    file_handler.setFormatter(formatter)
    root_logger.addHandler(file_handler)
    
    console_handler = logging.StreamHandler()
    console_handler.setLevel(detail_level)
    console_formatter = OneLineFormatter('[%(asctime)s] %(levelname)s:%(name)s:%(filename)s:%(funcName)s: %(message)s')
    console_handler.setFormatter(console_formatter)
    console_handler.addFilter(lambda record: not getattr(record, '_is_summary', False))
    
    q_handler = QueueHandler(log_queue)
    root_logger.addHandler(q_handler)
    
    global queue_listener
    queue_listener = QueueListener(log_queue, console_handler)
    queue_listener.start()
    
    if AggregatingHandler is not None:
        aggregator_handler = AggregatingHandler(level=detail_level)
        aggregator_handler.addFilter(lambda record: not getattr(record, '_is_summary', False))
        aggregator_formatter = OneLineFormatter('[%(asctime)s] %(levelname)s:%(name)s:%(filename)s:%(funcName)s: %(message)s')
        aggregator_handler.setFormatter(aggregator_formatter)
        root_logger.addHandler(aggregator_handler)

def setup_logger(module_name: str) -> logging.Logger:
    logger = logging.getLogger(module_name)
    logger.setLevel(detail_level)
    logger.propagate = True
    try:
        agg_handler = AggregatingHandler(level=detail_level)
        agg_handler.addFilter(lambda record: not getattr(record, '_is_summary', False))
        formatter = OneLineFormatter('[%(asctime)s] %(levelname)s:%(name)s:%(filename)s:%(funcName)s: %(message)s')
        agg_handler.setFormatter(formatter)
        logger.addHandler(agg_handler)
    except Exception as e:
        logger.error("모듈별 AggregatingHandler 추가 실패: %s", e)
    return logger

def shutdown_logging():
    root_logger = logging.getLogger()
    for handler in root_logger.handlers:
        try:
            if hasattr(handler, 'flush_aggregation_summary'):
                handler.flush_aggregation_summary()
        except Exception:
            pass
    global queue_listener
    if queue_listener is not None:
        queue_listener.stop()
        queue_listener = None
    logging.shutdown()

---

# logs/logging_util.py
import threading
import os
import glob
from logs.logger_config import setup_logger

class LoggingUtil:
    """
    LoggingUtil는 이벤트 로깅과 로그 파일 관리를 제공합니다.
    이벤트 발생 시 INFO/DEBUG 레벨 로그를 기록하며, clear_log_files()로 logs 폴더 내 .log 파일을 삭제합니다.
    또한, log_weekly_signal() 메서드를 통해 주간 전략 신호 이벤트를 별도로 로깅할 수 있습니다.
    """
    def __init__(self, module_name: str):
        self.module_name = module_name
        self.lock = threading.RLock()
        self.logger = setup_logger(module_name)

    def log_event(self, event_message: str) -> None:
        with self.lock:
            self.logger.debug(f"[{self.module_name}] Event: {event_message}")

    def log_summary(self) -> None:
        with self.lock:
            self.logger.debug(f"[{self.module_name}] Summary requested.")

    def log_weekly_signal(self, event_message: str) -> None:
        """
        주간 전략 신호 이벤트를 INFO 레벨로 로깅하며, 기록에 'is_weekly_signal' 플래그를 추가합니다.
        이를 통해 AggregatingHandler에서 별도로 집계할 수 있습니다.
        """
        with self.lock:
            self.logger.debug(f"[WEEKLY_SIGNAL] {event_message}", extra={'is_weekly_signal': True})

    @staticmethod
    def clear_log_files():
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        log_dir = os.path.join(base_dir, "logs")
        log_files = glob.glob(os.path.join(log_dir, "*.log"))
        for log_file in log_files:
            try:
                os.remove(log_file)
                print(f"Deleted log file: {log_file}")
            except Exception as e:
                print(f"Failed to remove log file {log_file}: {e}")
