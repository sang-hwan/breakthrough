[data module code]
# data/db/db_config.py
import os
from dotenv import load_dotenv

load_dotenv()

DATABASE: dict = {
    'user': os.getenv('DB_USER'),
    'password': os.getenv('DB_PASSWORD'),
    'host': os.getenv('DB_HOST'),
    'port': int(os.getenv('DB_PORT')),
    'dbname': os.getenv('DB_NAME')
}

---

# data/db/db_manager.py
from sqlalchemy import create_engine, text
from psycopg2.extras import execute_values
import pandas as pd
from typing import Any, Iterable, Dict
from data.db.db_config import DATABASE
from logs.logger_config import setup_logger

logger = setup_logger(__name__)

def insert_on_conflict(table: Any, conn: Any, keys: list, data_iter: Iterable) -> None:
    """
    Custom insertion method for pandas to_sql that handles conflicts on the 'timestamp' column.
    """
    try:
        raw_conn = conn.connection
        cur = raw_conn.cursor()
        values = list(data_iter)
        columns = ", ".join(keys)
        sql = f"INSERT INTO {table.name} ({columns}) VALUES %s ON CONFLICT (timestamp) DO NOTHING"
        execute_values(cur, sql, values)
        cur.close()
    except Exception as e:
        logger.error(f"insert_on_conflict 에러: {e}", exc_info=True)

def insert_ohlcv_records(df: pd.DataFrame, table_name: str = 'ohlcv_data', conflict_action: str = "DO NOTHING",
                         db_config: Dict[str, Any] = None, chunk_size: int = 10000) -> None:
    """
    Insert OHLCV records into the specified table, creating the table if it doesn't exist.
    """
    if db_config is None:
        db_config = DATABASE

    engine = create_engine(
        f"postgresql://{db_config['user']}:{db_config['password']}@"
        f"{db_config['host']}:{db_config['port']}/{db_config['dbname']}",
        pool_pre_ping=True
    )

    create_table_sql = text(f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            timestamp TIMESTAMP NOT NULL,
            open DOUBLE PRECISION,
            high DOUBLE PRECISION,
            low DOUBLE PRECISION,
            close DOUBLE PRECISION,
            volume DOUBLE PRECISION,
            PRIMARY KEY (timestamp)
        );
    """)
    try:
        with engine.begin() as conn:
            conn.execute(create_table_sql)
    except Exception as e:
        logger.error(f"테이블 생성 에러 ({table_name}): {e}", exc_info=True)
        return

    try:
        df = df.copy()
        df.reset_index(inplace=True)
        df.to_sql(
            table_name,
            engine,
            if_exists='append',
            index=False,
            method=insert_on_conflict,
            chunksize=chunk_size
        )
    except Exception as e:
        logger.error(f"데이터 저장 에러 ({table_name}): {e}", exc_info=True)

def fetch_ohlcv_records(table_name: str = 'ohlcv_data', start_date: str = None, end_date: str = None,
                        db_config: Dict[str, Any] = None) -> pd.DataFrame:
    """
    Fetch OHLCV records from the specified table within the given date range.
    """
    if db_config is None:
        db_config = DATABASE

    try:
        engine = create_engine(
            f"postgresql://{db_config['user']}:{db_config['password']}@"
            f"{db_config['host']}:{db_config['port']}/{db_config['dbname']}",
            pool_pre_ping=True
        )
    except Exception as e:
        logger.error(f"DB 엔진 생성 에러: {e}", exc_info=True)
        return pd.DataFrame()

    query = f"SELECT * FROM {table_name} WHERE 1=1"
    params = {}
    if start_date:
        query += " AND timestamp >= :start_date"
        params['start_date'] = start_date
    if end_date:
        query += " AND timestamp <= :end_date"
        params['end_date'] = end_date
    query += " ORDER BY timestamp"
    query = text(query)
    try:
        df = pd.read_sql(query, engine, params=params, parse_dates=['timestamp'])
        df.set_index('timestamp', inplace=True)
        return df
    except Exception as e:
        logger.error(f"데이터 로드 에러 ({table_name}): {e}", exc_info=True)
        return pd.DataFrame()

---

# data/ohlcv/ohlcv_aggregator.py
import pandas as pd
from ta.trend import SMAIndicator
from logs.logger_config import setup_logger

logger = setup_logger(__name__)

def aggregate_to_weekly(
    df: pd.DataFrame,
    compute_indicators: bool = True,
    resample_rule: str = "W-MON",
    label: str = "left",
    closed: str = "left",
    timezone: str = None,
    sma_window: int = 5
) -> pd.DataFrame:
    """
    Aggregate OHLCV data to a weekly frequency, and optionally compute technical indicators.
    Optionally, adjust timezone of the index.

    Parameters:
        df (pd.DataFrame): OHLCV data with a datetime index.
        compute_indicators (bool): Whether to compute additional indicators (weekly SMA, momentum, volatility).
        resample_rule (str): Resample rule string for pandas (default "W-MON" means weekly starting on Monday).
        label (str): Labeling convention for the resample (default "left").
        closed (str): Which side is closed (default "left").
        timezone (str): Optional timezone string (e.g., "UTC", "Asia/Seoul") to convert the index.
        sma_window (int): Window size for computing weekly SMA.

    Returns:
        pd.DataFrame: Weekly aggregated DataFrame with columns:
                      open, weekly_high, weekly_low, close, volume, and, if requested,
                      weekly_sma, weekly_momentum, weekly_volatility.
    """
    # Validate required columns
    required_columns = {"open", "high", "low", "close", "volume"}
    missing = required_columns - set(df.columns)
    if missing:
        logger.error(f"Input DataFrame is missing required columns: {missing}")
        return pd.DataFrame()

    # Ensure index is datetime type
    if not pd.api.types.is_datetime64_any_dtype(df.index):
        try:
            df.index = pd.to_datetime(df.index)
        except Exception as e:
            logger.error(f"Failed to convert index to datetime: {e}", exc_info=True)
            return pd.DataFrame()

    if df.empty:
        logger.error("Input DataFrame for aggregation is empty.")
        return df

    try:
        if timezone:
            # If timezone is specified and the index is naive, localize to UTC then convert
            if df.index.tz is None:
                df = df.tz_localize('UTC')
            df = df.tz_convert(timezone)
    except Exception as e:
        logger.error(f"Timezone conversion error: {e}", exc_info=True)

    try:
        weekly = df.resample(rule=resample_rule, label=label, closed=closed).agg({
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        })
    except Exception as e:
        logger.error(f"Resampling error: {e}", exc_info=True)
        return pd.DataFrame()

    if weekly.empty:
        logger.error("Aggregated weekly DataFrame is empty after resampling.")
        return weekly

    # Rename columns for easier reference in strategies
    weekly.rename(columns={'high': 'weekly_high', 'low': 'weekly_low'}, inplace=True)

    if compute_indicators:
        try:
            # Weekly SMA: using a parameterized window (default 5)
            sma_indicator = SMAIndicator(close=weekly['close'], window=sma_window, fillna=True)
            weekly['weekly_sma'] = sma_indicator.sma_indicator()
            # Weekly momentum: percentage change of the weekly close
            weekly['weekly_momentum'] = weekly['close'].pct_change() * 100
            # Weekly volatility: (weekly_high - weekly_low) / weekly_low, with fallback for division by zero
            weekly['weekly_volatility'] = weekly.apply(
                lambda row: (row['weekly_high'] - row['weekly_low']) / row['weekly_low']
                if row['weekly_low'] != 0 else 0.0, axis=1)
        except Exception as e:
            logger.error(f"Error computing weekly indicators: {e}", exc_info=True)

    return weekly

---

# data/ohlcv/ohlcv_fetcher.py
import ccxt
import pandas as pd
from datetime import datetime
import time
from logs.logger_config import setup_logger
from functools import lru_cache

logger = setup_logger(__name__)

@lru_cache(maxsize=32)
def fetch_historical_ohlcv_data(symbol: str, timeframe: str, start_date: str, 
                                limit_per_request: int = 1000, pause_sec: float = 1.0, 
                                exchange_id: str = 'binance', single_fetch: bool = False,
                                time_offset_ms: int = 1, max_retries: int = 3) -> pd.DataFrame:
    """
    Fetch historical OHLCV data for the specified symbol and timeframe starting from start_date.
    Uses caching to reduce duplicate API calls.
    """
    try:
        exchange_class = getattr(ccxt, exchange_id)
        exchange = exchange_class({'enableRateLimit': True})
        exchange.load_markets()
    except Exception as e:
        logger.error(f"Exchange '{exchange_id}' 초기화 에러: {e}", exc_info=True)
        return pd.DataFrame()

    try:
        since = exchange.parse8601(datetime.strptime(start_date, "%Y-%m-%d").isoformat())
    except Exception as e:
        logger.error(f"start_date ({start_date}) 파싱 에러: {e}", exc_info=True)
        return pd.DataFrame()

    ohlcv_list = []
    retry_count = 0
    while True:
        try:
            ohlcvs = exchange.fetch_ohlcv(symbol, timeframe, since=since, limit=limit_per_request)
        except Exception as e:
            logger.error(f"{symbol}의 {timeframe} 데이터 수집 에러: {e}", exc_info=True)
            retry_count += 1
            if retry_count >= max_retries:
                logger.error(f"최대 재시도({max_retries}) 초과로 {symbol} {timeframe} 데이터 수집 중단")
                break
            time.sleep(pause_sec)
            continue

        if not ohlcvs:
            break

        ohlcv_list.extend(ohlcvs)
        
        if single_fetch:
            break

        last_timestamp = ohlcvs[-1][0]
        since = last_timestamp + time_offset_ms

        if last_timestamp >= exchange.milliseconds():
            break

        time.sleep(pause_sec)

    if ohlcv_list:
        try:
            df = pd.DataFrame(ohlcv_list, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df.set_index('timestamp', inplace=True)
            return df.copy()  # Return a copy to prevent external modifications
        except Exception as e:
            logger.error(f"DataFrame 변환 에러: {e}", exc_info=True)
            return pd.DataFrame()
    else:
        logger.warning(f"{symbol} {timeframe}에 대한 데이터가 없습니다.")
        return pd.DataFrame()

@lru_cache(maxsize=32)
def fetch_latest_ohlcv_data(symbol: str, timeframe: str, limit: int = 500, exchange_id: str = 'binance') -> pd.DataFrame:
    """
    Fetch the latest OHLCV data for the specified symbol and timeframe.
    """
    try:
        exchange_class = getattr(ccxt, exchange_id)
        exchange = exchange_class({'enableRateLimit': True})
        exchange.load_markets()
    except Exception as e:
        logger.error(f"Exchange '{exchange_id}' 초기화 에러: {e}", exc_info=True)
        return pd.DataFrame()
    
    try:
        ohlcvs = exchange.fetch_ohlcv(symbol, timeframe, limit=limit)
    except Exception as e:
        logger.error(f"{symbol}의 {timeframe} 최신 데이터 수집 에러: {e}", exc_info=True)
        return pd.DataFrame()
    
    if ohlcvs:
        try:
            df = pd.DataFrame(ohlcvs, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df.set_index('timestamp', inplace=True)
            return df.copy()
        except Exception as e:
            logger.error(f"DataFrame 변환 에러: {e}", exc_info=True)
            return pd.DataFrame()
    else:
        logger.warning(f"{symbol} {timeframe}에 대한 최신 데이터가 없습니다.")
        return pd.DataFrame()

def get_top_market_cap_symbols(exchange_id: str = 'binance', quote_currency: str = 'USDT', 
                               required_start_date: str = "2018-01-01", count: int = 5, 
                               pause_sec: float = 1.0) -> list:
    """
    Retrieve top market cap symbols based on trading volume that have historical data
    starting from the required_start_date.
    """
    try:
        exchange_class = getattr(ccxt, exchange_id)
        exchange = exchange_class({'enableRateLimit': True})
        markets = exchange.load_markets()
    except Exception as e:
        logger.error(f"{exchange_id}에서 마켓 로드 에러: {e}", exc_info=True)
        return []

    usdt_symbols = [symbol for symbol in markets if symbol.endswith('/' + quote_currency)]
    
    try:
        tickers = exchange.fetch_tickers()
    except Exception as e:
        logger.error(f"티커 수집 에러: {e}", exc_info=True)
        tickers = {}

    symbol_volumes = []
    for symbol in usdt_symbols:
        ticker = tickers.get(symbol, {})
        volume = ticker.get('quoteVolume', 0)
        symbol_volumes.append((symbol, volume))
    
    symbol_volumes.sort(key=lambda x: x[1] if x[1] is not None else 0, reverse=True)
    
    valid_symbols = []
    for symbol, volume in symbol_volumes:
        df = fetch_historical_ohlcv_data(symbol, '1d', required_start_date, 
                                         limit_per_request=1, pause_sec=pause_sec, 
                                         exchange_id=exchange_id, single_fetch=True)
        if df.empty:
            continue
        first_timestamp = df.index.min()
        if first_timestamp > pd.to_datetime(required_start_date):
            continue
        valid_symbols.append(symbol)
        if len(valid_symbols) >= count:
            break

    if len(valid_symbols) < count:
        logger.warning(f"경고: {required_start_date} 이전 데이터가 있는 유효 심볼이 {len(valid_symbols)}개 밖에 없습니다.")
    return valid_symbols

---

# data/ohlcv/ohlcv_pipeline.py
import time
import threading
from typing import List, Optional
import concurrent.futures
from logs.logger_config import setup_logger
from data.ohlcv.ohlcv_fetcher import (
    fetch_historical_ohlcv_data,
    fetch_latest_ohlcv_data
)
from data.db.db_manager import insert_ohlcv_records

logger = setup_logger(__name__)

# In-memory cache for fetched OHLCV data (to avoid duplicate API calls)
_cache_lock = threading.Lock()
_ohlcv_cache: dict = {}

def collect_and_store_ohlcv_data(
    symbols: List[str],
    timeframes: List[str],
    use_historical: bool = True,
    start_date: Optional[str] = '2018-01-01 00:00:00',
    limit_per_request: int = 1000,
    latest_limit: int = 500,
    pause_sec: float = 1.0,
    table_name_format: str = "ohlcv_{symbol}_{timeframe}",
    exchange_id: str = 'binance',
    time_offset_ms: int = 1
) -> None:
    def process_symbol_tf(symbol: str, tf: str) -> None:
        key = (symbol, tf, use_historical, start_date, limit_per_request, latest_limit, exchange_id, time_offset_ms)
        with _cache_lock:
            if key in _ohlcv_cache:
                df = _ohlcv_cache[key]
                logger.debug(f"Cache hit for {symbol} {tf}")
            else:
                logger.debug(f"Cache miss for {symbol} {tf}, fetching data")
                if use_historical:
                    if not start_date:
                        raise ValueError("과거 데이터 수집 시 start_date가 필요합니다.")
                    df = fetch_historical_ohlcv_data(
                        symbol=symbol,
                        timeframe=tf,
                        start_date=start_date,
                        limit_per_request=limit_per_request,
                        pause_sec=pause_sec,
                        exchange_id=exchange_id,
                        single_fetch=False,
                        time_offset_ms=time_offset_ms
                    )
                else:
                    df = fetch_latest_ohlcv_data(
                        symbol=symbol,
                        timeframe=tf,
                        limit=latest_limit,
                        exchange_id=exchange_id
                    )
                _ohlcv_cache[key] = df
        if df.empty:
            logger.warning(f"[OHLCV PIPELINE] {symbol} - {tf} 데이터가 없습니다. 저장 건너뜁니다.")
            return
        table_name = table_name_format.format(symbol=symbol.replace('/', '').lower(), timeframe=tf)
        try:
            insert_ohlcv_records(df, table_name=table_name)
            logger.debug(f"Data inserted for {symbol} {tf} into table {table_name}")
        except Exception as e:
            logger.error(f"[OHLCV PIPELINE] 데이터 저장 에러 ({table_name}): {e}", exc_info=True)
        time.sleep(pause_sec)  # 짧은 대기 시간으로 API rate limit 준수

    tasks = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        for symbol in symbols:
            for tf in timeframes:
                tasks.append(executor.submit(process_symbol_tf, symbol, tf))
        concurrent.futures.wait(tasks)
