[data module code]
# data/db/db_config.py
import os
from dotenv import load_dotenv

load_dotenv()

DATABASE: dict = {
    'user': os.getenv('DB_USER'),
    'password': os.getenv('DB_PASSWORD'),
    'host': os.getenv('DB_HOST'),
    'port': int(os.getenv('DB_PORT')),
    'dbname': os.getenv('DB_NAME')
}

---

# data/db/db_manager.py
from sqlalchemy import create_engine, text
from psycopg2.extras import execute_values
import pandas as pd
from typing import Any, Iterable, Dict
from data.db.db_config import DATABASE
from logs.logger_config import setup_logger

logger = setup_logger(__name__)

def insert_on_conflict(table: Any, conn: Any, keys: list, data_iter: Iterable) -> None:
    """
    Custom insertion method for pandas to_sql that handles conflicts on the 'timestamp' column.
    """
    try:
        raw_conn = conn.connection
        cur = raw_conn.cursor()
        values = list(data_iter)
        columns = ", ".join(keys)
        sql_str = f"INSERT INTO {table.name} ({columns}) VALUES %s ON CONFLICT (timestamp) DO NOTHING"
        execute_values(cur, sql_str, values)
        cur.close()
    except Exception as e:
        logger.error(f"insert_on_conflict 에러: {e}", exc_info=True)

def insert_ohlcv_records(df: pd.DataFrame, table_name: str = 'ohlcv_data', conflict_action: str = "DO NOTHING",
                         db_config: Dict[str, Any] = None, chunk_size: int = 10000) -> None:
    """
    Insert OHLCV records into the specified table, creating the table if it doesn't exist.
    """
    if db_config is None:
        db_config = DATABASE

    engine = create_engine(
        f"postgresql://{db_config['user']}:{db_config['password']}@"
        f"{db_config['host']}:{db_config['port']}/{db_config['dbname']}",
        pool_pre_ping=True
    )

    create_table_sql = text(f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            timestamp TIMESTAMP NOT NULL,
            open DOUBLE PRECISION,
            high DOUBLE PRECISION,
            low DOUBLE PRECISION,
            close DOUBLE PRECISION,
            volume DOUBLE PRECISION,
            PRIMARY KEY (timestamp)
        );
    """)
    try:
        with engine.begin() as conn:
            conn.execute(create_table_sql)
    except Exception as e:
        logger.error(f"테이블 생성 에러 ({table_name}): {e}", exc_info=True)
        return

    try:
        df = df.copy()
        df.reset_index(inplace=True)
        df.to_sql(
            table_name,
            engine,
            if_exists='append',
            index=False,
            method=insert_on_conflict,
            chunksize=chunk_size
        )
    except Exception as e:
        logger.error(f"데이터 저장 에러 ({table_name}): {e}", exc_info=True)

def fetch_ohlcv_records(table_name: str = 'ohlcv_data', start_date: str = None, end_date: str = None,
                        db_config: Dict[str, Any] = None) -> pd.DataFrame:
    """
    Fetch OHLCV records from the specified table within the given date range.
    """
    if db_config is None:
        db_config = DATABASE

    try:
        engine = create_engine(
            f"postgresql://{db_config['user']}:{db_config['password']}@"
            f"{db_config['host']}:{db_config['port']}/{db_config['dbname']}",
            pool_pre_ping=True
        )
    except Exception as e:
        logger.error(f"DB 엔진 생성 에러: {e}", exc_info=True)
        return pd.DataFrame()

    query = f"SELECT * FROM {table_name} WHERE 1=1"
    params = {}
    if start_date:
        query += " AND timestamp >= :start_date"
        params['start_date'] = start_date
    if end_date:
        query += " AND timestamp <= :end_date"
        params['end_date'] = end_date
    query += " ORDER BY timestamp"
    query = text(query)
    try:
        df = pd.read_sql(query, engine, params=params, parse_dates=['timestamp'])
        df.set_index('timestamp', inplace=True)
        return df
    except Exception as e:
        logger.error(f"데이터 로드 에러 ({table_name}): {e}", exc_info=True)
        return pd.DataFrame()

def get_unique_symbol_list(db_config: Dict[str, Any] = None) -> list:
    """
    DB 내 public 스키마에서 테이블 이름을 조회하고,
    "ohlcv_{symbol}_{timeframe}" 형식에 맞는 테이블들을 분석하여 고유 symbol (예: BTC/USDT)을 반환합니다.
    만약 db_config가 제공되지 않으면 기본 DATABASE 설정을 사용합니다.
    """
    if db_config is None:
        db_config = DATABASE
    try:
        engine = create_engine(
            f"postgresql://{db_config['user']}:{db_config['password']}@"
            f"{db_config['host']}:{db_config['port']}/{db_config['dbname']}",
            pool_pre_ping=True
        )
        with engine.connect() as conn:
            result = conn.execute(text("SELECT tablename FROM pg_tables WHERE schemaname = 'public'"))
            tables = [row[0] for row in result]
    except Exception as e:
        logger.error(f"Error fetching table names: {e}", exc_info=True)
        return []

    symbol_set = set()
    for table in tables:
        # 테이블 이름이 ohlcv_ 로 시작하면 형식: ohlcv_{symbol}_{timeframe} 임을 가정
        if table.startswith("ohlcv_"):
            parts = table.split("_")
            if len(parts) >= 3:
                symbol_key = parts[1]  # 예: 'btcusdt'
                symbol_set.add(symbol_key)

    # symbol_key를 표준 형식(BTC/USDT)으로 변환 (간단 규칙: 뒤에 'usdt'가 있으면)
    symbols = []
    for s in symbol_set:
        if s.endswith("usdt"):
            base = s[:-4].upper()
            symbol = f"{base}/USDT"
            symbols.append(symbol)
        else:
            symbols.append(s.upper())
    return list(sorted(symbols))

---

# data/ohlcv/ohlcv_aggregator.py
import pandas as pd
from ta.trend import SMAIndicator
from logs.logger_config import setup_logger

logger = setup_logger(__name__)

def aggregate_to_weekly(
    df: pd.DataFrame,
    compute_indicators: bool = True,
    resample_rule: str = "W-MON",
    label: str = "left",
    closed: str = "left",
    timezone: str = None,
    sma_window: int = 5
) -> pd.DataFrame:
    """
    Aggregate OHLCV data to a weekly frequency, and optionally compute technical indicators.
    Optionally, adjust timezone of the index.

    Parameters:
        df (pd.DataFrame): OHLCV data with a datetime index.
        compute_indicators (bool): Whether to compute additional indicators (weekly SMA, momentum, volatility).
        resample_rule (str): Resample rule string for pandas (default "W-MON" means weekly starting on Monday).
        label (str): Labeling convention for the resample (default "left").
        closed (str): Which side is closed (default "left").
        timezone (str): Optional timezone string (e.g., "UTC", "Asia/Seoul") to convert the index.
        sma_window (int): Window size for computing weekly SMA.

    Returns:
        pd.DataFrame: Weekly aggregated DataFrame with columns:
                      open, weekly_high, weekly_low, close, volume, and, if requested,
                      weekly_sma, weekly_momentum, weekly_volatility.
    """
    # Validate required columns
    required_columns = {"open", "high", "low", "close", "volume"}
    missing = required_columns - set(df.columns)
    if missing:
        logger.error(f"Input DataFrame is missing required columns: {missing}", exc_info=True)
        return pd.DataFrame()

    # Ensure index is datetime type
    if not pd.api.types.is_datetime64_any_dtype(df.index):
        try:
            df.index = pd.to_datetime(df.index)
        except Exception as e:
            logger.error(f"Failed to convert index to datetime: {e}", exc_info=True)
            return pd.DataFrame()

    if df.empty:
        logger.error("Input DataFrame for aggregation is empty.", exc_info=True)
        return df

    try:
        if timezone:
            # If timezone is specified and the index is naive, localize to UTC then convert
            if df.index.tz is None:
                df = df.tz_localize('UTC')
            df = df.tz_convert(timezone)
    except Exception as e:
        logger.error(f"Timezone conversion error: {e}", exc_info=True)

    try:
        weekly = df.resample(rule=resample_rule, label=label, closed=closed).agg({
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        })
    except Exception as e:
        logger.error(f"Resampling error: {e}", exc_info=True)
        return pd.DataFrame()

    if weekly.empty:
        logger.error("Aggregated weekly DataFrame is empty after resampling.", exc_info=True)
        return weekly

    # Rename columns for easier reference in strategies
    weekly.rename(columns={'high': 'weekly_high', 'low': 'weekly_low'}, inplace=True)

    if compute_indicators:
        try:
            # Weekly SMA: using a parameterized window (default 5)
            sma_indicator = SMAIndicator(close=weekly['close'], window=sma_window, fillna=True)
            weekly['weekly_sma'] = sma_indicator.sma_indicator()
            # Weekly momentum: percentage change of the weekly close
            weekly['weekly_momentum'] = weekly['close'].pct_change() * 100
            # Weekly volatility: (weekly_high - weekly_low) / weekly_low, with fallback for division by zero
            weekly['weekly_volatility'] = weekly.apply(
                lambda row: (row['weekly_high'] - row['weekly_low']) / row['weekly_low']
                if row['weekly_low'] != 0 else 0.0, axis=1)
        except Exception as e:
            logger.error(f"Error computing weekly indicators: {e}", exc_info=True)
    return weekly

---

# data/ohlcv/ohlcv_fetcher.py
import ccxt
import pandas as pd
from datetime import datetime
import time
from logs.logger_config import setup_logger
from functools import lru_cache

logger = setup_logger(__name__)

def extract_onboard_date(market_info) -> str:
    """
    Extract onboardDate from the given market_info dictionary.
    Returns a string in "%Y-%m-%d %H:%M:%S" format.
    If unavailable or error occurs, returns the default "2018-01-01 00:00:00".
    """
    default_date = "2018-01-01 00:00:00"
    if market_info:
        onboard = market_info.get('info', {}).get('onboardDate')
        if onboard:
            try:
                onboard_dt = datetime.fromtimestamp(int(onboard) / 1000)
                return onboard_dt.strftime("%Y-%m-%d %H:%M:%S")
            except Exception:
                return default_date
    return default_date

@lru_cache(maxsize=32)
def fetch_historical_ohlcv_data(symbol: str, timeframe: str, start_date: str, 
                                limit_per_request: int = 1000, pause_sec: float = 1.0, 
                                exchange_id: str = 'binance', single_fetch: bool = False,
                                time_offset_ms: int = 1, max_retries: int = 3,
                                exchange_instance=None) -> pd.DataFrame:
    """
    Fetch historical OHLCV data for the specified symbol and timeframe starting from start_date.
    Uses caching to reduce duplicate API calls.
    
    If exchange_instance is provided, it will be reused instead of creating a new instance.
    """
    if exchange_instance is None:
        try:
            exchange_class = getattr(ccxt, exchange_id)
            exchange = exchange_class({'enableRateLimit': True, 'timeout': 30000})
            logger.debug(f"{exchange_id}: Loading markets...")
            exchange.load_markets()
            logger.debug(f"{exchange_id}: Markets loaded successfully.")
        except Exception as e:
            logger.error(f"Exchange '{exchange_id}' 초기화 에러: {e}", exc_info=True)
            return pd.DataFrame()
    else:
        exchange = exchange_instance

    try:
        # 만약 start_date가 "YYYY-MM-DD" 형식이면 시간 부분(" 00:00:00")을 추가합니다.
        if len(start_date.strip()) == 10:
            start_date += " 00:00:00"
        since = exchange.parse8601(datetime.strptime(start_date, "%Y-%m-%d %H:%M:%S").isoformat())
    except Exception as e:
        logger.error(f"start_date ({start_date}) 파싱 에러: {e}", exc_info=True)
        return pd.DataFrame()

    ohlcv_list = []
    retry_count = 0
    while True:
        try:
            ohlcvs = exchange.fetch_ohlcv(symbol, timeframe, since=since, limit=limit_per_request)
        except Exception as e:
            logger.error(f"{symbol}의 {timeframe} 데이터 수집 에러: {e}", exc_info=True)
            retry_count += 1
            if retry_count >= max_retries:
                logger.error(f"최대 재시도({max_retries}) 초과로 {symbol} {timeframe} 데이터 수집 중단")
                break
            time.sleep(pause_sec)
            continue

        if not ohlcvs:
            break

        ohlcv_list.extend(ohlcvs)
        
        if single_fetch:
            break

        last_timestamp = ohlcvs[-1][0]
        since = last_timestamp + time_offset_ms

        if last_timestamp >= exchange.milliseconds():
            break

        time.sleep(pause_sec)

    if ohlcv_list:
        try:
            df = pd.DataFrame(ohlcv_list, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df.set_index('timestamp', inplace=True)
            return df.copy()  # Return a copy to prevent external modifications
        except Exception as e:
            logger.error(f"DataFrame 변환 에러: {e}", exc_info=True)
            return pd.DataFrame()
    else:
        logger.warning(f"{symbol} {timeframe}에 대한 데이터가 없습니다.")
        return pd.DataFrame()

@lru_cache(maxsize=32)
def fetch_latest_ohlcv_data(symbol: str, timeframe: str, limit: int = 500, exchange_id: str = 'binance') -> pd.DataFrame:
    """
    Fetch the latest OHLCV data for the specified symbol and timeframe.
    """
    try:
        exchange_class = getattr(ccxt, exchange_id)
        exchange = exchange_class({'enableRateLimit': True, 'timeout': 30000})
        exchange.load_markets()
    except Exception as e:
        logger.error(f"Exchange '{exchange_id}' 초기화 에러: {e}", exc_info=True)
        return pd.DataFrame()
    
    try:
        ohlcvs = exchange.fetch_ohlcv(symbol, timeframe, limit=limit)
    except Exception as e:
        logger.error(f"{symbol}의 {timeframe} 최신 데이터 수집 에러: {e}", exc_info=True)
        return pd.DataFrame()
    
    if ohlcvs:
        try:
            df = pd.DataFrame(ohlcvs, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df.set_index('timestamp', inplace=True)
            return df.copy()
        except Exception as e:
            logger.error(f"DataFrame 변환 에러: {e}", exc_info=True)
            return pd.DataFrame()
    else:
        logger.warning(f"{symbol} {timeframe}에 대한 최신 데이터가 없습니다.")
        return pd.DataFrame()

def get_top_volume_symbols(exchange_id: str = 'binance', quote_currency: str = 'USDT', 
                           required_start_date: str = "2018-01-01", count: int = 5, 
                           pause_sec: float = 1.0) -> list:
    """
    Retrieve the top symbols based on trading volume (quoteVolume) that have historical data
    available since the required_start_date.
    
    This function creates a single ccxt exchange instance to load market data and tickers,
    sorts the USDT symbols by their quoteVolume in descending order, and filters out symbols
    without data prior to the required_start_date. It returns a list of tuples, where each tuple
    contains (symbol, onboard_date) as a string in "%Y-%m-%d %H:%M:%S" format.
    """
    try:
        exchange_class = getattr(ccxt, exchange_id)
        exchange = exchange_class({'enableRateLimit': True, 'timeout': 30000})
        logger.debug(f"{exchange_id}: Loading markets for top symbol retrieval...")
        markets = exchange.load_markets()
        logger.debug(f"{exchange_id}: Markets loaded, total symbols: {len(markets)}")
    except Exception as e:
        logger.error(f"{exchange_id}에서 마켓 로드 에러: {e}", exc_info=True)
        return []

    usdt_symbols = [symbol for symbol in markets if symbol.endswith('/' + quote_currency)]
    logger.debug(f"Found {len(usdt_symbols)} USDT symbols.")

    try:
        logger.debug("Fetching tickers...")
        tickers = exchange.fetch_tickers()
        logger.debug("Tickers fetched successfully.")
    except Exception as e:
        logger.error(f"티커 수집 에러: {e}", exc_info=True)
        tickers = {}

    symbol_volumes = []
    for symbol in usdt_symbols:
        ticker = tickers.get(symbol, {})
        volume = ticker.get('quoteVolume', 0)
        symbol_volumes.append((symbol, volume))
    
    symbol_volumes.sort(key=lambda x: x[1] if x[1] is not None else 0, reverse=True)
    
    valid_symbols = []
    for symbol, volume in symbol_volumes:
        df = fetch_historical_ohlcv_data(symbol, '1d', required_start_date, 
                                         limit_per_request=1, pause_sec=pause_sec, 
                                         exchange_id=exchange_id, single_fetch=True,
                                         exchange_instance=exchange)
        if df.empty:
            continue
        first_timestamp = df.index.min()
        if first_timestamp > pd.to_datetime(required_start_date):
            continue
        # onboardDate 추출
        market_info = exchange.markets.get(symbol)
        onboard_str = extract_onboard_date(market_info)
        valid_symbols.append((symbol, onboard_str))
        if len(valid_symbols) >= count:
            break

    if len(valid_symbols) < count:
        logger.warning(f"경고: {required_start_date} 이전 데이터가 있는 유효 심볼이 {len(valid_symbols)}개 밖에 없습니다.")
    return valid_symbols

def get_latest_onboard_date(symbols: list, exchange_id: str = 'binance') -> str:
    """
    For the given list of symbols, retrieve each symbol's onboardDate using extract_onboard_date,
    and return the latest date as a string in "%Y-%m-%d %H:%M:%S" format.
    If retrieval fails, returns the default "2018-01-01 00:00:00".
    """
    onboard_dates = []
    try:
        exchange_class = getattr(ccxt, exchange_id)
        exchange = exchange_class({'enableRateLimit': True, 'timeout': 30000})
        exchange.load_markets()
        for item in symbols:
            # symbols may be a list of tuples (symbol, onboard_date) or just symbol strings.
            if isinstance(item, tuple):
                symbol = item[0]
            else:
                symbol = item
            market_info = exchange.markets.get(symbol)
            onboard_str = extract_onboard_date(market_info)
            try:
                dt = datetime.strptime(onboard_str, "%Y-%m-%d %H:%M:%S")
                onboard_dates.append(dt)
            except Exception:
                onboard_dates.append(datetime.strptime("2018-01-01 00:00:00", "%Y-%m-%d %H:%M:%S"))
        if onboard_dates:
            latest_date = max(onboard_dates)
            return latest_date.strftime("%Y-%m-%d %H:%M:%S")
    except Exception as e:
        logger.error(f"Error in get_latest_onboard_date: {e}", exc_info=True)
    return "2018-01-01 00:00:00"

---

# data/ohlcv/ohlcv_pipeline.py
import time
import threading
from typing import List, Optional
import concurrent.futures
from logs.logger_config import setup_logger
from data.ohlcv.ohlcv_fetcher import (
    fetch_historical_ohlcv_data,
    fetch_latest_ohlcv_data
)
from data.db.db_manager import insert_ohlcv_records

logger = setup_logger(__name__)

# In-memory cache for fetched OHLCV data (to avoid duplicate API calls)
_cache_lock = threading.Lock()
_ohlcv_cache: dict = {}

def collect_and_store_ohlcv_data(
    symbols: List[str],
    timeframes: List[str],
    use_historical: bool = True,
    start_date: Optional[str] = '2018-01-01 00:00:00',
    limit_per_request: int = 1000,
    latest_limit: int = 500,
    pause_sec: float = 1.0,
    table_name_format: str = "ohlcv_{symbol}_{timeframe}",
    exchange_id: str = 'binance',
    time_offset_ms: int = 1
) -> None:
    def process_symbol_tf(symbol: str, tf: str) -> None:
        key = (symbol, tf, use_historical, start_date, limit_per_request, latest_limit, exchange_id, time_offset_ms)
        with _cache_lock:
            if key in _ohlcv_cache:
                df = _ohlcv_cache[key]
                logger.debug(f"Cache hit for {symbol} {tf}")
            else:
                logger.debug(f"Cache miss for {symbol} {tf}, fetching data")
                if use_historical:
                    if not start_date:
                        raise ValueError("과거 데이터 수집 시 start_date가 필요합니다.")
                    df = fetch_historical_ohlcv_data(
                        symbol=symbol,
                        timeframe=tf,
                        start_date=start_date,
                        limit_per_request=limit_per_request,
                        pause_sec=pause_sec,
                        exchange_id=exchange_id,
                        single_fetch=False,
                        time_offset_ms=time_offset_ms
                    )
                else:
                    df = fetch_latest_ohlcv_data(
                        symbol=symbol,
                        timeframe=tf,
                        limit=latest_limit,
                        exchange_id=exchange_id
                    )
                _ohlcv_cache[key] = df
        if df.empty:
            logger.warning(f"[OHLCV PIPELINE] {symbol} - {tf} 데이터가 없습니다. 저장 건너뜁니다.")
            return
        table_name = table_name_format.format(symbol=symbol.replace('/', '').lower(), timeframe=tf)
        try:
            insert_ohlcv_records(df, table_name=table_name)
            logger.debug(f"Data inserted for {symbol} {tf} into table {table_name}")
        except Exception as e:
            logger.error(f"[OHLCV PIPELINE] 데이터 저장 에러 ({table_name}): {e}", exc_info=True)
        time.sleep(pause_sec)  # 짧은 대기 시간으로 API rate limit 준수

    tasks = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        for symbol in symbols:
            for tf in timeframes:
                tasks.append(executor.submit(process_symbol_tf, symbol, tf))
        concurrent.futures.wait(tasks)
